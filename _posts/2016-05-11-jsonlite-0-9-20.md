---
layout: post
title: "Faster arrays and matrices in jsonlite 0.9.20"
category: posts
description: "Yesterday a new version of the jsonlite package was released to CRAN. This update includes no new features, it only introduces performance optimizations."
cover: "containers.jpg"
thumb: "mariokart.jpg"
---

Yesterday a new version of the [jsonlite](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html) package was released to CRAN. This update includes no new features, it only introduces performance optimizations.

## Large Matrices

The jsonlite package was already highly optimized for converting vectors and data frames to json. However earlier this year, Gregory Jefferis and Duncan Murdoch had found that conversion of large matrices as used by [rglwidget](https://cran.r-project.org/web/packages/rglwidget/index.html) was slower than expected.

Turned out this was indeed an edge case that I had overlooked. The new version of jsonlite fixes this problem and matrix conversion should be about 200 times as fast. Some technical details follow below. First a benchmark:


```r
# Old version!
> system.time(j<-toJSON(matrix(1L, ncol=3, nrow=50000)))
   user  system elapsed
  4.715   0.015   4.729

# New version!
> system.time(j<-toJSON(matrix(1L, ncol=3, nrow=50000)))
   user  system elapsed
  0.022   0.002   0.023
```

This artificial example (every field has the number 1) highlights the improvement. When converting matrices with actual data the difference will be smaller because significant time is spent number formatting actual doubles/integers. Luckily, this was already optimized in jsonlite a [while ago](https://www.opencpu.org/posts/jsonlite-release-0-9-13/).

## Technical Details

So what was the problem? The previous version of jsonlite had an elegant solution that would recurse through the dimensions of a matrix/array and apply json conversion on each of its elements. E.g. for a matrix (2D array) it would convert each row to json, and then combine the results. However it turns out that the `apply` call below is really slow.

```r
# Technical example, don't use this code !
x <- matrix(1L, ncol=3, nrow=50000)
rows <- apply(x, 1, jsonlite:::asJSON)
json <- jsonlite:::collapse(rows, indent = NA)
```

The new version exploits the fact that matrices and arrays are homogenous (i.e. all elements have the same type). It first removes the dimensions from the array using `c(x)` and converts all of the individual elements to json with a single call to `asJSON`.

```r
# Technical example, don't use this code !
str <- jsonlite:::asJSON(c(x), collapse = FALSE)
dim(str) <- dim(x)
rows <- apply(str, 1, jsonlite:::collapse, indent = NA)
json <- jsonlite:::collapse(rows, indent = NA)
```

This results in a significant speedup because `asJSON` is only called once rather than `n` times.

Now you might be thinking: can we avoid `apply` alltogether? Yes! For the important special case of 2 dimensional arrays (i.e matrices) jsonlite has has a complete solution in C. This is why `toJSON` on matrices is extra fast. For higher dimensional arrays it currently still uses the solution above, which performs pretty well. We might be able to optimize his a bit further by porting this as well, but working with high dimensional arrays in C makes my head hurt.
